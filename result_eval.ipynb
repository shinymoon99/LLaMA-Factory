{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 728, 1: 232, -1: 39, 2: 1})\n",
      "Counter({0: 839, -1: 82, 1: 79})\n"
     ]
    }
   ],
   "source": [
    "from utils.util import load_json_line_by_line\n",
    "from collections import Counter\n",
    "data = load_json_line_by_line(\"./output\\ECIhardprompt\\generated_predictions (7).jsonl\")\n",
    "def get_label_from_str(text):\n",
    "    return text.split(\"\\n\", 1)[0]    \n",
    "def find_match_substring(text):\n",
    "    if \"none\" in text:\n",
    "        return 0\n",
    "    elif \"caused by\" in text:\n",
    "        return -1\n",
    "    elif \"cause\" in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "#label2index 1 -1 0\n",
    "label2index = {\"event1 and event2 have no causal relationship\":0,\"event1 caused by event2\":-1,\"event1 cause event2\":1}\n",
    "ground_truth = [label2index[d[\"label\"]] for d in data]\n",
    "predictions = [d[\"predict\"] for d in data]\n",
    "true_predictions = [get_label_from_str(p) for p in predictions]\n",
    "predictions1 = [find_match_substring(p) for p in true_predictions]\n",
    "print(Counter(predictions1))\n",
    "print(Counter(ground_truth))\n",
    "#get content before \\n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Precision=0.86, Recall=0.96, F1 Score=0.91\n",
      "Label 1: Precision=0.11, Recall=0.84, F1 Score=0.20\n",
      "Label -1: Precision=1.00, Recall=1.00, F1 Score=1.00\n",
      "Label 2: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
      "0.658\n",
      "Counter({0: 839, -1: 82, 1: 79})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict  \n",
    "#错误，误用\n",
    "def calculate_per_class_metrics(ground_truth, predictions):  \n",
    "    # 创建字典来存储每个类别的TP, FP, FN计数  \n",
    "    tp_fp_fn = defaultdict(lambda: {'TP': 0, 'FP': 0, 'FN': 0})  \n",
    "      \n",
    "    # 遍历所有样本  \n",
    "    for gt, pred in zip(ground_truth, predictions):  \n",
    "        # 如果预测正确，增加TP计数  \n",
    "        if gt == pred:  \n",
    "            tp_fp_fn[gt]['TP'] += 1  \n",
    "        # 如果预测错误但预测为正例，增加FP计数  \n",
    "        elif pred != -1 and gt != pred:  # 假设-1是未分类或填充值，根据实际情况调整  \n",
    "            tp_fp_fn[pred]['FP'] += 1  \n",
    "        # 如果预测错误且实际为正例但预测为负例或未分类，增加FN计数  \n",
    "        elif gt != -1 and gt != pred:  \n",
    "            tp_fp_fn[gt]['FN'] += 1  \n",
    "      \n",
    "    # 计算每个类别的精确度、召回率和F1分数  \n",
    "    metrics_per_class = {}  \n",
    "    for label, counts in tp_fp_fn.items():  \n",
    "        tp = counts['TP']  \n",
    "        fp = counts['FP']  \n",
    "        fn = counts['FN']  \n",
    "          \n",
    "        # 避免除以零的情况  \n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0  \n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0  \n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0  \n",
    "          \n",
    "        metrics_per_class[label] = {  \n",
    "            'Precision': precision,  \n",
    "            'Recall': recall,  \n",
    "            'F1 Score': f1_score  \n",
    "        }  \n",
    "\n",
    "    # 返回整体的P、R和F1         \n",
    "    return metrics_per_class  \n",
    "def calculate_accuracy(ground_truth, predictions):  \n",
    "    # 使用列表推导式计算正确分类的样本数  \n",
    "    correct_predictions = sum(1 for gt, pred in zip(ground_truth, predictions) if gt == pred)  \n",
    "      \n",
    "    # 计算准确率  \n",
    "    accuracy = correct_predictions / len(ground_truth)  \n",
    "      \n",
    "    # 返回准确率  \n",
    "    return accuracy    \n",
    "\n",
    "metrics = calculate_per_class_metrics(ground_truth, predictions1)  \n",
    "for label, metric_values in metrics.items():  \n",
    "    print(f\"Label {label}: Precision={metric_values['Precision']:.2f}, Recall={metric_values['Recall']:.2f}, F1 Score={metric_values['F1 Score']:.2f}\")\n",
    "print(calculate_accuracy(ground_truth,predictions1))\n",
    "print(Counter(ground_truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Precision: 0.44285714285714284\n",
      "Recall: 0.124\n",
      "F1-score: 0.19375\n",
      "Support: 250\n",
      "\n",
      "Class: 1\n",
      "Precision: 0.575187969924812\n",
      "Recall: 0.6095617529880478\n",
      "F1-score: 0.5918762088974855\n",
      "Support: 502\n",
      "\n",
      "Class: 2\n",
      "Precision: 0.40350877192982454\n",
      "Recall: 0.368\n",
      "F1-score: 0.3849372384937238\n",
      "Support: 250\n",
      "\n",
      "Class: 3\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "Support: 0\n",
      "\n",
      "Class: 4\n",
      "Precision: 0.49933716526846056\n",
      "Recall: 0.4281437125748503\n",
      "F1-score: 0.4409118428043599\n",
      "Support: None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\llmprocedure\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\miniconda3\\envs\\llmprocedure\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def calculate_prf_per_class(predictions, ground_truth):\n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(ground_truth, predictions, average=None)\n",
    "    oprecision, orecall, of1, osupport =precision_recall_fscore_support(ground_truth, predictions, average=\"weighted\")\n",
    "    # Create a dictionary to store results for each class\n",
    "    class_results = {}\n",
    "    for i in range(len(precision)):\n",
    "        class_results[i] = {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1-score': f1[i],\n",
    "            'support': support[i]\n",
    "        }\n",
    "    class_results[len(precision)]={\n",
    "            'precision': oprecision,\n",
    "            'recall': orecall,\n",
    "            'f1-score': of1,\n",
    "            'support': osupport\n",
    "        }\n",
    "    return class_results\n",
    "\n",
    "# Example usage:\n",
    "# Replace these with your actual predictions and ground truth lists\n",
    "\n",
    "class_results = calculate_prf_per_class(predictions1, ground_truth)\n",
    "\n",
    "# Display results for each class\n",
    "for class_label, metrics in class_results.items():\n",
    "    print(\"Class:\", class_label)\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1-score:\", metrics['f1-score'])\n",
    "    print(\"Support:\", metrics['support'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 548, 1: 225, 2: 164, -1: 63})\n",
      "Counter({0: 500, 1: 250, -1: 250})\n",
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from utils.util import load_json_line_by_line\n",
    "from collections import Counter\n",
    "import re\n",
    "data = load_json_line_by_line(\"./output\\ECIhardprompt\\generated_predictions (13).jsonl\")\n",
    "def get_label_from_str(text):\n",
    "    return text.split(\".\", 1)[0]    \n",
    "def extract_label(input_string):\n",
    "    # Define a regular expression pattern to match the label substring\n",
    "    pattern = r'<a>(.*?)</a>'\n",
    "    \n",
    "    # Use re.findall to find all matches of the pattern in the input string\n",
    "    matches = re.findall(pattern, input_string)\n",
    "    \n",
    "    # Return the first match if found, or None if no match is found\n",
    "    return matches[0] if matches else None\n",
    "def find_match_substring(text):\n",
    "    if \"none\"==text:\n",
    "        return 0\n",
    "    elif \"caused by\"==text:\n",
    "        return -1\n",
    "    elif \"cause\"==text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "#label2index 1 -1 0\n",
    "label2index = {\"event1 and event2 have no causal relationship\":0,\"event1 caused by event2\":-1,\"event1 cause event2\":1}\n",
    "ground_truth = [label2index[d[\"label\"]] for d in data]\n",
    "predictions = [d[\"predict\"] for d in data]\n",
    "true_predictions = [extract_label(p) for p in predictions]\n",
    "predictions1 = [find_match_substring(p) for p in true_predictions]\n",
    "print(Counter(predictions1))\n",
    "print(Counter(ground_truth))\n",
    "print(len(predictions1))\n",
    "print(len(ground_truth))\n",
    "print(len(data))\n",
    "#get content before \\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: 0\n",
      "Precision: 0.3333333333333333\n",
      "Recall: 0.084\n",
      "F1-score: 0.134185303514377\n",
      "Support: 250\n",
      "\n",
      "Class: 1\n",
      "Precision: 0.5620437956204379\n",
      "Recall: 0.616\n",
      "F1-score: 0.5877862595419847\n",
      "Support: 500\n",
      "\n",
      "Class: 2\n",
      "Precision: 0.38222222222222224\n",
      "Recall: 0.344\n",
      "F1-score: 0.3621052631578947\n",
      "Support: 250\n",
      "\n",
      "Class: 3\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1-score: 0.0\n",
      "Support: 0\n",
      "\n",
      "Class: 4\n",
      "Precision: 0.4599107866991078\n",
      "Recall: 0.415\n",
      "F1-score: 0.4179657714390603\n",
      "Support: None\n",
      "\n",
      "Counter({0: 500, 1: 250, -1: 250})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\miniconda3\\envs\\llmprocedure\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "D:\\miniconda3\\envs\\llmprocedure\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "class_results = calculate_prf_per_class(predictions1, ground_truth)\n",
    "#print(calculate_accuracy(ground_truth,predictions1))\n",
    "# Display results for each class\n",
    "for class_label, metrics in class_results.items():\n",
    "    print(\"Class:\", class_label)\n",
    "    print(\"Precision:\", metrics['precision'])\n",
    "    print(\"Recall:\", metrics['recall'])\n",
    "    print(\"F1-score:\", metrics['f1-score'])\n",
    "    print(\"Support:\", metrics['support'])\n",
    "    print()\n",
    "\n",
    "print(Counter(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been written.\n"
     ]
    }
   ],
   "source": [
    "import csv  \n",
    "from collections import Counter  \n",
    "  \n",
    "# 假设 class_results 是已经计算好的字典，其结构如下：  \n",
    "# class_results = {  \n",
    "#     'class1': {'precision': 0.8, 'recall': 0.9, 'f1-score': 0.85, 'support': 100},  \n",
    "#     'class2': {'precision': 0.7, 'recall': 0.8, 'f1-score': 0.75, 'support': 200},  \n",
    "#     # ... 更多类别  \n",
    "# }  \n",
    "  \n",
    "# 假设 ground_truth 是一个列表，包含了所有的真实标签  \n",
    "# ground_truth = ['class1', 'class2', 'class1', 'class2', ...]  \n",
    "  \n",
    "# 创建一个列表，用于存储CSV的行  \n",
    "csv_rows = []  \n",
    "  \n",
    "# 标题行  \n",
    "header = ['Class', 'Precision', 'Recall', 'F1-score', 'Support']  \n",
    "csv_rows.append(header)  \n",
    "  \n",
    "# 遍历 class_results 字典  \n",
    "for class_label, metrics in class_results.items():  \n",
    "    row = [class_label, \"{:.2f}\".format(metrics['precision']*100), \"{:.2f}\".format(metrics['recall']*100), \"{:.2f}\".format(metrics['f1-score']*100), metrics['support']]  \n",
    "    csv_rows.append(row)  \n",
    "  \n",
    "# 添加 ground_truth 的统计信息  \n",
    "ground_truth_counter = Counter(ground_truth)  \n",
    "for class_label, count in ground_truth_counter.items():  \n",
    "    row = [class_label, 'N/A', 'N/A', 'N/A', 'Support: ' + str(count)]  \n",
    "    csv_rows.append(row)  \n",
    "  \n",
    "# 打开一个文件以写入CSV  \n",
    "with open('./output/results2.csv', 'w', newline='') as csvfile:  \n",
    "    writer = csv.writer(csvfile)  \n",
    "    # 写入所有行  \n",
    "    writer.writerows(csv_rows)  \n",
    "  \n",
    "print(\"CSV file has been written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"./data\\ECIbalance1000.json\",\"r\") as f:\n",
    "    data =json.load(f)\n",
    "print(len(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
