{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 728, 1: 232, -1: 39, 2: 1})\n",
      "Counter({0: 839, -1: 82, 1: 79})\n"
     ]
    }
   ],
   "source": [
    "from utils.util import load_json_line_by_line\n",
    "from collections import Counter\n",
    "data = load_json_line_by_line(\"./output\\ECIhardprompt\\generated_predictions (7).jsonl\")\n",
    "def get_label_from_str(text):\n",
    "    return text.split(\"\\n\", 1)[0]    \n",
    "def find_match_substring(text):\n",
    "    if \"none\" in text:\n",
    "        return 0\n",
    "    elif \"caused by\" in text:\n",
    "        return -1\n",
    "    elif \"cause\" in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "#label2index 1 -1 0\n",
    "label2index = {\"event1 and event2 have no causal relationship\":0,\"event1 caused by event2\":-1,\"event1 cause event2\":1}\n",
    "ground_truth = [label2index[d[\"label\"]] for d in data]\n",
    "predictions = [d[\"predict\"] for d in data]\n",
    "true_predictions = [get_label_from_str(p) for p in predictions]\n",
    "predictions1 = [find_match_substring(p) for p in true_predictions]\n",
    "print(Counter(predictions1))\n",
    "print(Counter(ground_truth))\n",
    "#get content before \\n\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Precision=0.86, Recall=0.96, F1 Score=0.91\n",
      "Label 1: Precision=0.11, Recall=0.84, F1 Score=0.20\n",
      "Label -1: Precision=1.00, Recall=1.00, F1 Score=1.00\n",
      "Label 2: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
      "0.658\n",
      "Counter({0: 839, -1: 82, 1: 79})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict  \n",
    "  \n",
    "def calculate_per_class_metrics(ground_truth, predictions):  \n",
    "    # 创建字典来存储每个类别的TP, FP, FN计数  \n",
    "    tp_fp_fn = defaultdict(lambda: {'TP': 0, 'FP': 0, 'FN': 0})  \n",
    "      \n",
    "    # 遍历所有样本  \n",
    "    for gt, pred in zip(ground_truth, predictions):  \n",
    "        # 如果预测正确，增加TP计数  \n",
    "        if gt == pred:  \n",
    "            tp_fp_fn[gt]['TP'] += 1  \n",
    "        # 如果预测错误但预测为正例，增加FP计数  \n",
    "        elif pred != -1 and gt != pred:  # 假设-1是未分类或填充值，根据实际情况调整  \n",
    "            tp_fp_fn[pred]['FP'] += 1  \n",
    "        # 如果预测错误且实际为正例但预测为负例或未分类，增加FN计数  \n",
    "        elif gt != -1 and gt != pred:  \n",
    "            tp_fp_fn[gt]['FN'] += 1  \n",
    "      \n",
    "    # 计算每个类别的精确度、召回率和F1分数  \n",
    "    metrics_per_class = {}  \n",
    "    for label, counts in tp_fp_fn.items():  \n",
    "        tp = counts['TP']  \n",
    "        fp = counts['FP']  \n",
    "        fn = counts['FN']  \n",
    "          \n",
    "        # 避免除以零的情况  \n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0  \n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0  \n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0  \n",
    "          \n",
    "        metrics_per_class[label] = {  \n",
    "            'Precision': precision,  \n",
    "            'Recall': recall,  \n",
    "            'F1 Score': f1_score  \n",
    "        }  \n",
    "\n",
    "    # 返回整体的P、R和F1         \n",
    "    return metrics_per_class  \n",
    "def calculate_accuracy(ground_truth, predictions):  \n",
    "    # 使用列表推导式计算正确分类的样本数  \n",
    "    correct_predictions = sum(1 for gt, pred in zip(ground_truth, predictions) if gt == pred)  \n",
    "      \n",
    "    # 计算准确率  \n",
    "    accuracy = correct_predictions / len(ground_truth)  \n",
    "      \n",
    "    # 返回准确率  \n",
    "    return accuracy    \n",
    "\n",
    "metrics = calculate_per_class_metrics(ground_truth, predictions1)  \n",
    "for label, metric_values in metrics.items():  \n",
    "    print(f\"Label {label}: Precision={metric_values['Precision']:.2f}, Recall={metric_values['Recall']:.2f}, F1 Score={metric_values['F1 Score']:.2f}\")\n",
    "print(calculate_accuracy(ground_truth,predictions1))\n",
    "print(Counter(ground_truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 348, 2: 232, -1: 224, 1: 196})\n",
      "Counter({0: 839, -1: 82, 1: 79})\n"
     ]
    }
   ],
   "source": [
    "from utils.util import load_json_line_by_line\n",
    "from collections import Counter\n",
    "data = load_json_line_by_line(\"./output\\ECIhardprompt\\generated_predictions (9).jsonl\")\n",
    "def get_label_from_str(text):\n",
    "    return text.split(\".\", 1)[0]    \n",
    "def find_match_substring(text):\n",
    "    if \"no causal\" in text:\n",
    "        return 0\n",
    "    elif \"caused by\" in text:\n",
    "        return -1\n",
    "    elif \"cause\" in text:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "#label2index 1 -1 0\n",
    "label2index = {\"event1 and event2 have no causal relationship\":0,\"event1 caused by event2\":-1,\"event1 cause event2\":1}\n",
    "ground_truth = [label2index[d[\"label\"]] for d in data]\n",
    "predictions = [d[\"predict\"] for d in data]\n",
    "true_predictions = [get_label_from_str(p) for p in predictions]\n",
    "predictions1 = [find_match_substring(p) for p in true_predictions]\n",
    "print(Counter(predictions1))\n",
    "print(Counter(ground_truth))\n",
    "#get content before \\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: Precision=0.85, Recall=0.61, F1 Score=0.71\n",
      "Label 1: Precision=0.11, Recall=0.60, F1 Score=0.18\n",
      "Label 2: Precision=0.00, Recall=0.00, F1 Score=0.00\n",
      "Label -1: Precision=1.00, Recall=1.00, F1 Score=1.00\n",
      "0.339\n",
      "Counter({0: 839, -1: 82, 1: 79})\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_per_class_metrics(ground_truth, predictions1)  \n",
    "for label, metric_values in metrics.items():  \n",
    "    print(f\"Label {label}: Precision={metric_values['Precision']:.2f}, Recall={metric_values['Recall']:.2f}, F1 Score={metric_values['F1 Score']:.2f}\")\n",
    "print(calculate_accuracy(ground_truth,predictions1))\n",
    "print(Counter(ground_truth))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
